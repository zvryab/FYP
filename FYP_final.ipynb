{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzbPmIAB3/wckwEkzHhYGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zvryab/FYP/blob/main/FYP_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Year Project: Enhancing HCI using AI.**"
      ],
      "metadata": {
        "id": "EqnRUYGutAGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 1\n",
        "\n",
        "Main library is LangChain, which provides the framework for connecting the LLM to external tools like databases.\n",
        "\n",
        "Langchain-core for fundamental objects like prompts and memory, and langchain-community for external integrations, such as PostgreSQL database connectors.\n",
        "\n",
        "Updated pip, setuptools, and wheel to the latest versions to prevent installation or compatibility issues that could arise when using newer Python packages."
      ],
      "metadata": {
        "id": "xtTqN5TMtJ_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9F2bkpxhf_M"
      },
      "outputs": [],
      "source": [
        "!pip install langchain #the main langchain framework\n",
        "!pip install langchain-core #core building blocks of langchain\n",
        "!pip install langchain-community\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RUN TERMINAL\n",
        "\n",
        "You can do this on the bottom left if you have Colab Pro. Else search online for an alternative solution. The solution I had was not very successful and caused around 2-3 weeks delay so prepare for trial and error or pay for that ease of use.\n",
        "\n",
        "Once terminal is open, run the terminal text, each command is enclosed between the forward dash.\n",
        "\n",
        "Wait for ollama install, then when llama2 is running, you can pull any model.\n",
        "\n",
        "llama3.1, 3.2, 3.3 should be free but vary in size."
      ],
      "metadata": {
        "id": "HWhbcoap084L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terminal Text:\n",
        "\n",
        "/curl -fsSL https://ollama.com/install.sh | sh /ollama serve & ollama run llama2 /ollama pull llama3.1"
      ],
      "metadata": {
        "id": "GC6A0l0whqBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONNECT LANGCHAIN TO LLM\n",
        "\n",
        "This code connects the LangChain framework to the local LLM instance hosted via Ollama.\n",
        "I specify which model to use, here Llama 3.1, but it can be changed to newer versions like 3.3 for better performance if storage allows.\n",
        "\n",
        "The base_url is the Ngrok link that tunnels from Colab to my local machine.\n",
        "Setting temperature to 0 ensures that the LLM generates deterministic outputs, which is crucial for reliable SQL query generation, avoiding hallucinations or random variability."
      ],
      "metadata": {
        "id": "S6blED403-j4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm = Ollama( #LLM object inside ollama class\n",
        "    model=\"llama3.1\",  # or whicever model I pulled, use 3.3 for better results\n",
        "    base_url=\"[your ngrok url to connect to Colab goes here]",
        "    temperature=0, #controls randomness of LLM, you can refer to the testing section for more information (0= deterministic vs creative if higher such as 0.7)\n",
        ")\n"
      ],
      "metadata": {
        "id": "mn_QQekhhnP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Memory feature [unavaliable]\n",
        "\n",
        "Imported ConversationBufferMemory to allow the system to remember previous user queries and system responses during a session.\n",
        "\n",
        "ConversationChain is then used to link the language model with the memory, enabling multi-turn conversations rather than treating each input as completely independent.\n",
        "\n",
        "Feature not tested and requires further research."
      ],
      "metadata": {
        "id": "ie6bvPi_67I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory #memory management class for storing conversations\n",
        "from langchain.chains import ConversationChain    #imports a chain class that connects an LLM to a memory system"
      ],
      "metadata": {
        "id": "rP4R7vwGhvo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize memory to store conversation context\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "FONDyW56hyZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Database\n",
        "\n",
        "psycopg2 is a PostgreSQL adaptor for python so it allows code to send SQL commands AND recieve those results.\n",
        "\n",
        "sqlalchemy is a database tool kit plus ORM (object-relational mapper) to support the database connection in a structured way.\n",
        "\n",
        "The database connection is then setup and the SQLDatabase(engine) is to allow Langchain to understand the database through almost a structured abstraction so that it has an understanding of the schema/columns to make the queries with that given context."
      ],
      "metadata": {
        "id": "Hfa2wcTf7uzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2 sqlalchemy langchain ollama"
      ],
      "metadata": {
        "id": "XUdbZbXhh02o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community sqlalchemy psycopg2-binary\n",
        "\n",
        "from sqlalchemy import create_engine #create_engine opens connection to database\n",
        "from langchain_community.llms import Ollama #connects code to llm hosted via ollama - possible redundancy\n",
        "from langchain_community.agent_toolkits import create_sql_agent #creates an sql agent which carries out sql queries\n",
        "from langchain_community.utilities import SQLDatabase #gives langchain the ability to query the database\n",
        "from langchain.agents.agent_types import AgentType #specifies the type of langchain agent to use\n",
        "\n",
        "#connects database, all data can be found on ngrok UI on terminal\n",
        "DB_HOST = \"7.tcp.eu.ngrok.io\"\n",
        "DB_PORT = \"11662\"\n",
        "DB_NAME = \"sampledb\"\n",
        "DB_USER = \"postgres\"\n",
        "\n",
        "engine = create_engine(f\"postgresql://{DB_USER}@{DB_HOST}:{DB_PORT}/{DB_NAME}\") #this actually creates that connection using the inputs from variables above\n",
        "\n",
        "\n",
        "db = SQLDatabase(engine) #creates a structure abstraction to understand the context of the database"
      ],
      "metadata": {
        "id": "uB_3vSVxh8mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompting\n",
        "\n",
        "Below are supprotive classes, utilities and components to enhance the prompting feature.\n",
        "\n",
        "[Note: These were taken from some brief research online and their impact/usage is not 100% defined here so continue further research on their impact]"
      ],
      "metadata": {
        "id": "k4TfFu6EvvCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.sql_database.prompt import PROMPT_SUFFIX, _mysql_prompt\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "tJXOL8OEh_5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SQL AGENT\n",
        "\n",
        "This section builds and connects the core of my system â€” a prompt-driven SQL generator backed by an LLM agent.\n",
        "\n",
        "I tested both LangChain's default SQL prompt and a custom-designed prompt tailored to PostgreSQL syntax and behaviour. You can try both and are labelled as 'LangChain PROMPT' and 'CUSTOM PROMPT'\n",
        "\n",
        "These prompts, combined with the database schema and a conversation aware memory system, were passed to a LangChain agent using the ZERO_SHOT_REACT_DESCRIPTION type. This gives the model flexibility to reason and generate SQL based solely on input and schema, mimicking real conversation flow.\n",
        "\n",
        "Error-handling and verbosity were enabled to allow debugging and improve model trustworthiness."
      ],
      "metadata": {
        "id": "FMQG2c5F1GMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = Ollama(model=\"llama3.1\")\n",
        "\n",
        "#LangChain Prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"table_info\"],\n",
        "    template=_mysql_prompt + PROMPT_SUFFIX, #uses those pre-built langchain sql prompts\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True) #combines llm and prompt into a pipeline to take inputs\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "#Custom Prompt\n",
        "custom_sql_prompt = PromptTemplate(   #custom built prompt to add our own specific instructions to ensure accuracy\n",
        "    input_variables=[\"input\", \"table_info\"],\n",
        "    template=\"\"\"\n",
        "You are an expert SQL generator designed to interact with a PostgreSQL database.\n",
        "Generate a syntactically correct SQL query to answer the given natural language question.\n",
        "\n",
        "Only output the SQL query. Do not include explanations, comments, or intermediate steps.\n",
        "\n",
        "Schema:\n",
        "{table_info}\n",
        "\n",
        "Question:\n",
        "{input}\n",
        "\n",
        "SQL Query:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "#create SQL agent\n",
        "agent_executor = create_sql_agent(\n",
        "    llm=llm,\n",
        "    db=db,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, #makes decision from a single prompt\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True, #try to recover rathen than crash\n",
        "    memory=memory\n",
        "    #prefix= custom_prefix\n",
        ")\n"
      ],
      "metadata": {
        "id": "Kstrrl3JiCQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interface\n",
        "\n",
        "We install Gradio as a temporary UI for users to interact with (which includes a shareable link) rather than locally testing on this notebook."
      ],
      "metadata": {
        "id": "wr9GrbUo1j9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "clWNKsu3iGXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Visualisation\n",
        "\n",
        "Attempted data visualisation but did not get results so continue this development to see if a solution can be found.\n",
        "\n",
        "Intended result: The code checks if the data is visualisable (even better with two columns as the tutorial followed supported this) and then outputs it.\n",
        "\n",
        "[Note: Code is not thoroughly annotated due to limited time spent on development ]"
      ],
      "metadata": {
        "id": "siYGXBcd1yM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib plotly #plotting library in python for graphs etc"
      ],
      "metadata": {
        "id": "Si7K_NAwiIne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # for bar charts\n",
        "import pandas as pd # to structure sql results into a dataframe\n",
        "from sqlalchemy import text #pass raw sql into sqlalchemy safely\n",
        "\n",
        "def visualize_query(query):\n",
        "    try:\n",
        "        # Execute the query and load results into a DataFrame\n",
        "        with engine.connect() as conn:\n",
        "            result = conn.execute(text(query))\n",
        "            data = result.fetchall()\n",
        "            columns = result.keys()\n",
        "\n",
        "        # Create a DataFrame from the query result (which is sql)\n",
        "        df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "        # Basic visualization: Bar chart if there are two columns (label and value)\n",
        "        if df.shape[1] == 2:\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.bar(df.iloc[:, 0], df.iloc[:, 1], color='skyblue')\n",
        "            plt.title(\"Query Result Visualisation\")\n",
        "            plt.xlabel(df.columns[0])\n",
        "            plt.ylabel(df.columns[1])\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot as an image\n",
        "            plt.savefig(\"query_result.png\")\n",
        "            plt.close()\n",
        "            return \"query_result.png\"\n",
        "        else:\n",
        "            return \"Visualisation not supported for this query format.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error in visualisation: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "E10ILJeKiKmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio Continued...\n",
        "\n",
        "I built a web-based interface using Gradio that enables users to interact with my system in plain language.\n",
        "Users can choose whether they want a text-based response or a visualised chart.\n",
        "\n",
        "The interface supports natural language questions, which are translated to SQL by the LangChain agent. The SQL is executed against the database, and the output is either displayed as raw text or a bar chart.\n",
        "\n",
        "This design improves the accessibility and usability of the system, especially for non-technical users, which aligns with the goal of enhancing human-computer interaction."
      ],
      "metadata": {
        "id": "AnkVUCuB52M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr #builds simple and shareable web interface\n",
        "\n",
        "def sql_query_agent(input_text, output_type): #this runs everytime a query is sent by a user\n",
        "    try:\n",
        "\n",
        "        result = agent_executor.run(input_text)\n",
        "\n",
        "                # If the output type is \"Visualisation\", send this to\n",
        "        if output_type == \"Graph\":\n",
        "            image_path = visualize_query(result)\n",
        "            if image_path.startswith(\"Error\"):\n",
        "                return image_path\n",
        "            return gr.Image.update(value=image_path)\n",
        "\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\" #basic error handling to return error string\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=sql_query_agent,  #binds interface to the sql agent\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"SQL Query\"),\n",
        "        gr.Dropdown(choices=[\"Text\", \"Graph\"], label=\"Output Type\", value=\"Text\") #lets user choose between text or graph output\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Query Result\"),\n",
        "    title=\"FYP: SQL Query Agent with Visualisation\",\n",
        "    description=\"Ask a question about your database, and I will generate and execute the SQL query for you. The agent remembers the context of previous questions.\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "ErbBnGl3iNpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch() #launches the interface"
      ],
      "metadata": {
        "id": "6v7No1NmiRyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVFR-1A_iTtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
